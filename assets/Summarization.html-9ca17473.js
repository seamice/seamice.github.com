import{_ as c}from"./plugin-vue_export-helper-c27b6911.js";import{r as i,o as p,c as u,b as n,d as e,e as s,w as t,f as d}from"./app-bcbb7a66.js";const m={},k={href:"https://colab.research.google.com/drive/1al92FMOhPry8m20sqcF40w5au8AH5LPT?usp=sharing",target:"_blank",rel:"noopener noreferrer"},h=n("span",{style:{color:"red"}},"(Please do not made public)",-1),b={href:"https://colab.research.google.com/drive/1IVOY5bo8AK7gu2svt2rLpeIlYVpSeLIV?usp=sharing",target:"_blank",rel:"noopener noreferrer"},_=n("span",{style:{color:"red"}},"(Please do not made public)",-1),v={href:"https://colab.research.google.com/drive/1E6UyNTrt40_wMDa7K2ZYrx4-hIymoqcf?usp=sharing",target:"_blank",rel:"noopener noreferrer"},g=n("hr",null,null,-1),y=n("p",null,[e("The table below is the summarization for "),n("span",{style:{color:"orange"}},"text summarization"),e(" and "),n("span",{style:{color:"orange"}},"key entity extraction"),e(". The next plan is to validate most of them find the best model and fine-tune them to fill our own needs.")],-1),f=n("thead",null,[n("tr",null,[n("th",null,"Name"),n("th",null,"Website"),n("th",null,"Type")])],-1),w=n("td",null,"GPT2",-1),x={href:"https://github.com/openai/gpt-2",target:"_blank",rel:"noopener noreferrer"},z=n("td",null,"OpenSource",-1),T=n("td",null,"XLNet",-1),P={href:"https://github.com/zihangdai/xlnet",target:"_blank",rel:"noopener noreferrer"},S=n("td",null,"OpenSource",-1),L=n("td",null,[n("span",{style:{color:"orange"}},"BERT")],-1),A={href:"https://github.com/google-research/bert",target:"_blank",rel:"noopener noreferrer"},G={href:"https://pypi.org/project/bert-extractive-summarizer/",target:"_blank",rel:"noopener noreferrer"},I=n("td",null,"OpenSource",-1),V=n("td",null,[n("span",{style:{color:"orange"}},"KeyBERT")],-1),N={href:"https://github.com/MaartenGr/KeyBERT",target:"_blank",rel:"noopener noreferrer"},q=n("td",null,"OpenSource",-1),F=n("td",null,"TextRank",-1),R={href:"https://github.com/summanlp/textrank",target:"_blank",rel:"noopener noreferrer"},C={href:"https://github.com/letiantian/TextRank4ZH",target:"_blank",rel:"noopener noreferrer"},W=n("td",null,"OpenSource",-1),B=n("tr",null,[n("td",null,"TF-IDF"),n("td"),n("td",null,"Unknow")],-1),E=n("td",null,"Word2Vec",-1),O={href:"https://github.com/danielfrg/word2vec",target:"_blank",rel:"noopener noreferrer"},M=n("td",null,"OpenSource",-1),D=n("td",null,"Gensim",-1),K={href:"https://github.com/piskvorky/gensim",target:"_blank",rel:"noopener noreferrer"},j=n("td",null,"OpenSource",-1),H=n("td",null,"Sumy",-1),Y={href:"https://github.com/miso-belica/sumy",target:"_blank",rel:"noopener noreferrer"},X=n("td",null,"OpenSource",-1),U=n("td",null,"NLTK",-1),Z={href:"https://github.com/nltk/nltk",target:"_blank",rel:"noopener noreferrer"},Q=n("td",null,"OpenSource",-1),J=n("td",null,"T5",-1),$={href:"https://github.com/google-research/t5x",target:"_blank",rel:"noopener noreferrer"},nn=n("td",null,"OpenSource",-1),en=n("td",null,[n("span",{style:{color:"orange"}},"GPT-3~4")],-1),sn={href:"https://github.com/openai/openai-cookbook",target:"_blank",rel:"noopener noreferrer"},an={href:"https://platform.openai.com/docs/models/overview",target:"_blank",rel:"noopener noreferrer"},tn=n("td",null,"Commercial",-1),on=n("td",null,"AWS Service",-1),ln={href:"https://aws.amazon.com/bedrock/jurassic/",target:"_blank",rel:"noopener noreferrer"},rn=n("td",null,"Commercial",-1),cn=n("td",null,[n("span",{style:{color:"orange"}},"PaLM")],-1),pn={href:"https://developers.generativeai.google/tutorials/text_quickstart",target:"_blank",rel:"noopener noreferrer"},un=n("td",null,"Commerical",-1),dn=n("hr",null,null,-1),mn=n("h2",{id:"annotated-bibliography",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#annotated-bibliography","aria-hidden":"true"},"#"),e(" Annotated bibliography")],-1),kn={href:"https://medium.com/analytics-vidhya/text-summarization-using-bert-gpt2-xlnet-5ee80608e961",target:"_blank",rel:"noopener noreferrer"},hn={class:"hint-container info"},bn=n("p",{class:"hint-container-title"},"Info",-1),_n=n("p",null,"Very general text to describe summarization.",-1),vn={style:{color:"orange","font-weight":"bold"}},gn={href:"https://jalammar.github.io/illustrated-gpt2/",target:"_blank",rel:"noopener noreferrer"},yn={href:"https://towardsdatascience.com/xlnet-a-clever-language-modeling-solution-ab41e87798b0",target:"_blank",rel:"noopener noreferrer"},fn={href:"https://utomorezadwi.medium.com/bert-extractive-summarizer-vs-word2vec-extractive-summarizer-which-one-is-better-and-faster-c6d6d172cb91",target:"_blank",rel:"noopener noreferrer"},wn={class:"hint-container info"},xn=n("p",{class:"hint-container-title"},"Info",-1),zn=n("p",null,"Very general text to describe summarization.",-1),Tn={style:{color:"orange","font-weight":"bold"}},Pn={href:"https://medium.com/data-science-in-your-pocket/text-summarization-using-textrank-in-nlp-4bce52c5b390",target:"_blank",rel:"noopener noreferrer"},Sn={href:"https://towardsdatascience.com/text-summarization-using-tf-idf-e64a0644ace3",target:"_blank",rel:"noopener noreferrer"},Ln={href:"https://medium.com/jatana/unsupervised-text-summarization-using-sentence-embeddings-adb15ce83db1",target:"_blank",rel:"noopener noreferrer"},An={href:"https://www.holisticseo.digital/python-seo/summarize/",target:"_blank",rel:"noopener noreferrer"},Gn=n("div",{class:"hint-container info"},[n("p",{class:"hint-container-title"},"Info"),n("p",null,"Very general description fro summarization.")],-1),In={href:"https://www.turing.com/kb/5-powerful-text-summarization-techniques-in-python",target:"_blank",rel:"noopener noreferrer"},Vn=n("div",{class:"language-python line-numbers-mode","data-ext":"py"},[n("pre",{class:"language-python"},[n("code",null,[n("span",{class:"token keyword"},"from"),e(" gensim"),n("span",{class:"token punctuation"},"."),e("summarization"),n("span",{class:"token punctuation"},"."),e("summarizer "),n("span",{class:"token keyword"},"import"),e(` summarize
`),n("span",{class:"token keyword"},"from"),e(" gensim"),n("span",{class:"token punctuation"},"."),e("summarization "),n("span",{class:"token keyword"},"import"),e(` keywords
`),n("span",{class:"token keyword"},"import"),e(` wikipedia
`),n("span",{class:"token keyword"},"import"),e(` en_core_web_sm

`),n("span",{class:"token comment"},"# To import the wikipedia content:"),e(`
wikisearch `),n("span",{class:"token operator"},"="),e(" wikipedia"),n("span",{class:"token punctuation"},"."),e("page"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'""'),n("span",{class:"token punctuation"},")"),e(`
wikicontent `),n("span",{class:"token operator"},"="),e(" wikisearch"),n("span",{class:"token punctuation"},"."),e(`content
nlp `),n("span",{class:"token operator"},"="),e(" en_core_web_sm"),n("span",{class:"token punctuation"},"."),e("load"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),e(`
doc `),n("span",{class:"token operator"},"="),e(" nlp"),n("span",{class:"token punctuation"},"("),e("wikicontent"),n("span",{class:"token punctuation"},")"),e(`

`),n("span",{class:"token comment"},"# To summarize based on percentage:"),e(`
summ_per `),n("span",{class:"token operator"},"="),e(" summarize"),n("span",{class:"token punctuation"},"("),e("wikicontent"),n("span",{class:"token punctuation"},","),e(" ratio "),n("span",{class:"token operator"},"="),e(),n("span",{class:"token string"},'""'),n("span",{class:"token punctuation"},")"),e(`
`),n("span",{class:"token keyword"},"print"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"Percent summary"'),n("span",{class:"token punctuation"},")"),e(`
`),n("span",{class:"token keyword"},"print"),n("span",{class:"token punctuation"},"("),e("summ_per"),n("span",{class:"token punctuation"},")"),e(`

`),n("span",{class:"token comment"},"#To summarize based on word count:"),e(`
summ_words `),n("span",{class:"token operator"},"="),e(" summarize"),n("span",{class:"token punctuation"},"("),e("wikicontent"),n("span",{class:"token punctuation"},","),e(" word_count "),n("span",{class:"token operator"},"="),e(),n("span",{class:"token string"},'""'),n("span",{class:"token punctuation"},")"),e(`
`),n("span",{class:"token keyword"},"print"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"Word count summary"'),n("span",{class:"token punctuation"},")"),e(`
`),n("span",{class:"token keyword"},"print"),n("span",{class:"token punctuation"},"("),e("summ_words"),n("span",{class:"token punctuation"},")"),e(`
`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"})])],-1),Nn=n("p",null,[n("em",null,[n("strong",null,"LexRank")]),e(": LexRank is a graphical-based summarizer.")],-1),qn=n("div",{class:"language-python line-numbers-mode","data-ext":"py"},[n("pre",{class:"language-python"},[n("code",null,[n("span",{class:"token keyword"},"from"),e(" sumy"),n("span",{class:"token punctuation"},"."),e("summarizers"),n("span",{class:"token punctuation"},"."),e("lex_rank "),n("span",{class:"token keyword"},"import"),e(` LexRankSummarizer
summarizer_lex `),n("span",{class:"token operator"},"="),e(" LexRankSummarizer"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),e(`

`),n("span",{class:"token comment"},"# Summarize using sumy LexRank"),e(`
summary`),n("span",{class:"token operator"},"="),e(" summarizer_lex"),n("span",{class:"token punctuation"},"("),e("parser"),n("span",{class:"token punctuation"},"."),e("document"),n("span",{class:"token punctuation"},","),e(),n("span",{class:"token number"},"2"),n("span",{class:"token punctuation"},")"),e(`
lex_summary`),n("span",{class:"token operator"},"="),n("span",{class:"token string"},'""'),e(`
`),n("span",{class:"token keyword"},"for"),e(" sentence "),n("span",{class:"token keyword"},"in"),e(" summary"),n("span",{class:"token punctuation"},":"),e(`
    lex_summary`),n("span",{class:"token operator"},"+="),n("span",{class:"token builtin"},"str"),n("span",{class:"token punctuation"},"("),e("sentence"),n("span",{class:"token punctuation"},")"),e(`
`),n("span",{class:"token keyword"},"print"),n("span",{class:"token punctuation"},"("),e("lex_summary"),n("span",{class:"token punctuation"},")"),e(`
`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"})])],-1),Fn=n("p",null,[n("em",null,[n("strong",null,"Luhn")]),e(": Developed by an IBM researcher of the same name, Luhn is one of the oldest summarization algorithms and ranks sentences based on a frequency criterion for words.")],-1),Rn=n("div",{class:"language-python line-numbers-mode","data-ext":"py"},[n("pre",{class:"language-python"},[n("code",null,[n("span",{class:"token keyword"},"from"),e(" sumy"),n("span",{class:"token punctuation"},"."),e("summarizers"),n("span",{class:"token punctuation"},"."),e("luhn "),n("span",{class:"token keyword"},"import"),e(` LuhnSummarizer
summarizer_1 `),n("span",{class:"token operator"},"="),e(" LuhnSummarizer"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),e(`
summary_1 `),n("span",{class:"token operator"},"="),e("summarizer_1"),n("span",{class:"token punctuation"},"("),e("parser"),n("span",{class:"token punctuation"},"."),e("document"),n("span",{class:"token punctuation"},","),e(),n("span",{class:"token number"},"2"),n("span",{class:"token punctuation"},")"),e(`

`),n("span",{class:"token keyword"},"for"),e(" sentence "),n("span",{class:"token keyword"},"in"),e(" summary_1"),n("span",{class:"token punctuation"},":"),e(`
    `),n("span",{class:"token keyword"},"print"),n("span",{class:"token punctuation"},"("),e("sentence"),n("span",{class:"token punctuation"},")"),e(`
`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"})])],-1),Cn=n("p",null,[n("em",null,[n("strong",null,"LSA")]),e(": Latent semantic analysis is an automated method of summarization that utilizes term frequency with singular value decomposition. It has become one of the most used summarizers in recent years.")],-1),Wn=n("div",{class:"language-python line-numbers-mode","data-ext":"py"},[n("pre",{class:"language-python"},[n("code",null,[n("span",{class:"token keyword"},"from"),e(" sumy"),n("span",{class:"token punctuation"},"."),e("summarizers"),n("span",{class:"token punctuation"},"."),e("lsa "),n("span",{class:"token keyword"},"import"),e(` LsaSummarizer
summarizer_lsa `),n("span",{class:"token operator"},"="),e(" LsaSummarizer"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),e(`

`),n("span",{class:"token comment"},"# Summarize using sumy LSA"),e(`
summary `),n("span",{class:"token operator"},"="),e("summarizer_lsa"),n("span",{class:"token punctuation"},"("),e("parser"),n("span",{class:"token punctuation"},"."),e("document"),n("span",{class:"token punctuation"},","),n("span",{class:"token number"},"2"),n("span",{class:"token punctuation"},")"),e(`
lsa_summary`),n("span",{class:"token operator"},"="),n("span",{class:"token string"},'""'),e(`

`),n("span",{class:"token keyword"},"for"),e(" sentence "),n("span",{class:"token keyword"},"in"),e(" summary"),n("span",{class:"token punctuation"},":"),e(`
    lsa_summary`),n("span",{class:"token operator"},"+="),n("span",{class:"token builtin"},"str"),n("span",{class:"token punctuation"},"("),e("sentence"),n("span",{class:"token punctuation"},")"),e(`
`),n("span",{class:"token keyword"},"print"),n("span",{class:"token punctuation"},"("),e("lsa_summary"),n("span",{class:"token punctuation"},")"),e(`
`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"})])],-1),Bn=n("p",null,[n("em",null,[n("strong",null,"TextRank")]),e(": And last but not least, there is TextRank which works exactly the same as in Gensim.")],-1),En=n("div",{class:"language-python line-numbers-mode","data-ext":"py"},[n("pre",{class:"language-python"},[n("code",null,[n("span",{class:"token comment"},"# Load Packages"),e(`
`),n("span",{class:"token keyword"},"from"),e(" sumy"),n("span",{class:"token punctuation"},"."),e("parsers"),n("span",{class:"token punctuation"},"."),e("plaintext "),n("span",{class:"token keyword"},"import"),e(` PlaintextParser
`),n("span",{class:"token keyword"},"from"),e(" sumy"),n("span",{class:"token punctuation"},"."),e("nlp"),n("span",{class:"token punctuation"},"."),e("tokenizers "),n("span",{class:"token keyword"},"import"),e(` Tokenizer

`),n("span",{class:"token comment"},"# For Strings"),e(`
parser `),n("span",{class:"token operator"},"="),e(" PlaintextParser"),n("span",{class:"token punctuation"},"."),e("from_string"),n("span",{class:"token punctuation"},"("),e("text"),n("span",{class:"token punctuation"},","),e("Tokenizer"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"english"'),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},")"),e(`
`),n("span",{class:"token keyword"},"from"),e(" sumy"),n("span",{class:"token punctuation"},"."),e("summarizers"),n("span",{class:"token punctuation"},"."),e("text_rank "),n("span",{class:"token keyword"},"import"),e(` TextRankSummarizer

`),n("span",{class:"token comment"},"# Summarize using sumy TextRank"),e(`
summarizer `),n("span",{class:"token operator"},"="),e(" TextRankSummarizer"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),e(`
summary `),n("span",{class:"token operator"},"="),e("summarizer_4"),n("span",{class:"token punctuation"},"("),e("parser"),n("span",{class:"token punctuation"},"."),e("document"),n("span",{class:"token punctuation"},","),n("span",{class:"token number"},"2"),n("span",{class:"token punctuation"},")"),e(`
text_summary`),n("span",{class:"token operator"},"="),n("span",{class:"token string"},'""'),e(`

`),n("span",{class:"token keyword"},"for"),e(" sentence "),n("span",{class:"token keyword"},"in"),e(" summary"),n("span",{class:"token punctuation"},":"),e(`
    text_summary`),n("span",{class:"token operator"},"+="),n("span",{class:"token builtin"},"str"),n("span",{class:"token punctuation"},"("),e("sentence"),n("span",{class:"token punctuation"},")"),e(`
`),n("span",{class:"token keyword"},"print"),n("span",{class:"token punctuation"},"("),e("text_summary"),n("span",{class:"token punctuation"},")"),e(`
`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"})])],-1),On=n("p",null,"The 'Natural Language Toolkit' is an NLP-based toolkit in Python that helps with text summarization.",-1),Mn=n("div",{class:"language-python line-numbers-mode","data-ext":"py"},[n("pre",{class:"language-python"},[n("code",null,[n("span",{class:"token keyword"},"import"),e(` nltk
`),n("span",{class:"token keyword"},"from"),e(" nltk"),n("span",{class:"token punctuation"},"."),e("corpus "),n("span",{class:"token keyword"},"import"),e(` stopwords
`),n("span",{class:"token keyword"},"from"),e(" nltk"),n("span",{class:"token punctuation"},"."),e("tokenize "),n("span",{class:"token keyword"},"import"),e(" word_tokenize"),n("span",{class:"token punctuation"},","),e(` sent_tokenize

Input your text `),n("span",{class:"token keyword"},"for"),e(" summarizing below"),n("span",{class:"token punctuation"},":"),e(`

text `),n("span",{class:"token operator"},"="),e(),n("span",{class:"token triple-quoted-string string"},'""" """'),e(`

`),n("span",{class:"token comment"},"# Next, you need to tokenize the text:"),e(`
stopWords `),n("span",{class:"token operator"},"="),e(),n("span",{class:"token builtin"},"set"),n("span",{class:"token punctuation"},"("),e("stopwords"),n("span",{class:"token punctuation"},"."),e("words"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"english"'),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},")"),e(`
words `),n("span",{class:"token operator"},"="),e(" word_tokenize"),n("span",{class:"token punctuation"},"("),e("text"),n("span",{class:"token punctuation"},")"),e(`

`),n("span",{class:"token comment"},"# Now, you will need to create a frequency table to keep a score of each word:"),e(`
freqTable `),n("span",{class:"token operator"},"="),e(),n("span",{class:"token builtin"},"dict"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),e(`
`),n("span",{class:"token keyword"},"for"),e(" word "),n("span",{class:"token keyword"},"in"),e(" words"),n("span",{class:"token punctuation"},":"),e(`
    word `),n("span",{class:"token operator"},"="),e(" word"),n("span",{class:"token punctuation"},"."),e("lower"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),e(`
    `),n("span",{class:"token keyword"},"if"),e(" word "),n("span",{class:"token keyword"},"in"),e(" stopWords"),n("span",{class:"token punctuation"},":"),e(`
        `),n("span",{class:"token keyword"},"continue"),e(`
    `),n("span",{class:"token keyword"},"if"),e(" word "),n("span",{class:"token keyword"},"in"),e(" freqTable"),n("span",{class:"token punctuation"},":"),e(`
        freqTable`),n("span",{class:"token punctuation"},"["),e("word"),n("span",{class:"token punctuation"},"]"),e(),n("span",{class:"token operator"},"+="),e(),n("span",{class:"token number"},"1"),e(`
    `),n("span",{class:"token keyword"},"else"),n("span",{class:"token punctuation"},":"),e(`
        freqTable`),n("span",{class:"token punctuation"},"["),e("word"),n("span",{class:"token punctuation"},"]"),e(),n("span",{class:"token operator"},"="),e(),n("span",{class:"token number"},"1"),e(`

`),n("span",{class:"token comment"},"# Next, create a dictionary to keep the score of each sentence:"),e(`
sentences `),n("span",{class:"token operator"},"="),e(" sent_tokenize"),n("span",{class:"token punctuation"},"("),e("text"),n("span",{class:"token punctuation"},")"),e(`
sentenceValue `),n("span",{class:"token operator"},"="),e(),n("span",{class:"token builtin"},"dict"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),e(`

`),n("span",{class:"token keyword"},"for"),e(" sentence "),n("span",{class:"token keyword"},"in"),e(" sentences"),n("span",{class:"token punctuation"},":"),e(`
    `),n("span",{class:"token keyword"},"for"),e(" word"),n("span",{class:"token punctuation"},","),e(" freq "),n("span",{class:"token keyword"},"in"),e(" freqTable"),n("span",{class:"token punctuation"},"."),e("items"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},":"),e(`
        `),n("span",{class:"token keyword"},"if"),e(" word "),n("span",{class:"token keyword"},"in"),e(" sentence"),n("span",{class:"token punctuation"},"."),e("lower"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},":"),e(`
            `),n("span",{class:"token keyword"},"if"),e(" word "),n("span",{class:"token keyword"},"in"),e(" sentence"),n("span",{class:"token punctuation"},"."),e("lower"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},":"),e(`
                `),n("span",{class:"token keyword"},"if"),e(" sentence "),n("span",{class:"token keyword"},"in"),e(" sentenceValue"),n("span",{class:"token punctuation"},":"),e(`
                    sentenceValue`),n("span",{class:"token punctuation"},"["),e("sentence"),n("span",{class:"token punctuation"},"]"),e(),n("span",{class:"token operator"},"+="),e(` freq
                `),n("span",{class:"token keyword"},"else"),n("span",{class:"token punctuation"},":"),e(`
                    sentenceValue`),n("span",{class:"token punctuation"},"["),e("sentence"),n("span",{class:"token punctuation"},"]"),e(),n("span",{class:"token operator"},"="),e(` freq
sumValues `),n("span",{class:"token operator"},"="),e(),n("span",{class:"token number"},"0"),e(`
`),n("span",{class:"token keyword"},"for"),e(" sentence "),n("span",{class:"token keyword"},"in"),e(" sentenceValue"),n("span",{class:"token punctuation"},":"),e(`
    sumValues `),n("span",{class:"token operator"},"+="),e(" sentenceValue"),n("span",{class:"token punctuation"},"["),e("sentence"),n("span",{class:"token punctuation"},"]"),e(`

`),n("span",{class:"token comment"},"# Now, we define the average value from the original text as such:"),e(`
average `),n("span",{class:"token operator"},"="),e(),n("span",{class:"token builtin"},"int"),n("span",{class:"token punctuation"},"("),e("sumValues "),n("span",{class:"token operator"},"/"),e(),n("span",{class:"token builtin"},"len"),n("span",{class:"token punctuation"},"("),e("sentenceValue"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},")"),e(`

`),n("span",{class:"token comment"},"# And lastly, we need to store the sentences into our summary:"),e(`
summary `),n("span",{class:"token operator"},"="),e(),n("span",{class:"token string"},"''"),e(`
`),n("span",{class:"token keyword"},"for"),e(" sentence "),n("span",{class:"token keyword"},"in"),e(" sentences"),n("span",{class:"token punctuation"},":"),e(`
    `),n("span",{class:"token keyword"},"if"),e(),n("span",{class:"token punctuation"},"("),e("sentence "),n("span",{class:"token keyword"},"in"),e(" sentenceValue"),n("span",{class:"token punctuation"},")"),e(),n("span",{class:"token keyword"},"and"),e(),n("span",{class:"token punctuation"},"("),e("sentenceValue"),n("span",{class:"token punctuation"},"["),e("sentence"),n("span",{class:"token punctuation"},"]"),e(),n("span",{class:"token operator"},">"),e(),n("span",{class:"token punctuation"},"("),n("span",{class:"token number"},"1.2"),e(),n("span",{class:"token operator"},"*"),e(" average"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},":"),e(`
        summary `),n("span",{class:"token operator"},"+="),e(),n("span",{class:"token string"},'" "'),e(),n("span",{class:"token operator"},"+"),e(` sentence
`),n("span",{class:"token keyword"},"print"),n("span",{class:"token punctuation"},"("),e("summary"),n("span",{class:"token punctuation"},")"),e(`
`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"})])],-1),Dn=n("p",null,"To make use of Google’s T5 summarizer, there are a few prerequisites.",-1),Kn=n("p",null,"First, you will need to install PyTorch and Hugging Face’s Transformers. You can install the transformers using the code below:",-1),jn=n("p",null,[n("code",null,"pip install transformers")],-1),Hn=n("p",null,"Next, import PyTorch along with the AutoTokenizer and AutoModelWithLMHead objects:",-1),Yn=n("div",{class:"language-python line-numbers-mode","data-ext":"py"},[n("pre",{class:"language-python"},[n("code",null,[n("span",{class:"token keyword"},"import"),e(` torch
`),n("span",{class:"token keyword"},"from"),e(" transformers"),n("span",{class:"token punctuation"},","),e(),n("span",{class:"token keyword"},"import"),e(" AutoTokenizer"),n("span",{class:"token punctuation"},","),e(` AutoModelWithLMHead
`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"}),n("div",{class:"line-number"})])],-1),Xn=n("p",null,"Next, you need to initialize the tokenizer model:",-1),Un=n("div",{class:"language-python line-numbers-mode","data-ext":"py"},[n("pre",{class:"language-python"},[n("code",null,[e("tokenizer "),n("span",{class:"token operator"},"="),e(" AutoTokenizer"),n("span",{class:"token punctuation"},"."),e("from_pretrained"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},"'t5-base'"),n("span",{class:"token punctuation"},")"),e(`
model `),n("span",{class:"token operator"},"="),e(" AutoModelWithLMHead"),n("span",{class:"token punctuation"},"."),e("from_pretrained"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},"'t5-base'"),n("span",{class:"token punctuation"},","),e(" return_dict"),n("span",{class:"token operator"},"="),n("span",{class:"token boolean"},"True"),n("span",{class:"token punctuation"},")"),e(`
`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"}),n("div",{class:"line-number"})])],-1),Zn=n("p",null,"From here, you can use any data you like to summarize. Once you have gathered your data, input the code below to tokenize it:",-1),Qn=n("div",{class:"language-python line-numbers-mode","data-ext":"py"},[n("pre",{class:"language-python"},[n("code",null,[e("inputs "),n("span",{class:"token operator"},"="),e(" tokenizer"),n("span",{class:"token punctuation"},"."),e("encode"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"summarize: "'),e(),n("span",{class:"token operator"},"+"),e(" text"),n("span",{class:"token punctuation"},","),e(`
    return_tensors`),n("span",{class:"token operator"},"="),n("span",{class:"token string"},"'pt'"),n("span",{class:"token punctuation"},","),e(`
    max_length`),n("span",{class:"token operator"},"="),n("span",{class:"token number"},"512"),n("span",{class:"token punctuation"},","),e(`
    truncation`),n("span",{class:"token operator"},"="),n("span",{class:"token boolean"},"True"),n("span",{class:"token punctuation"},")"),e(`
`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"})])],-1),Jn=n("p",null,"Now, you can generate the summary by using the model.generate function on T5:",-1),$n=n("div",{class:"language-python line-numbers-mode","data-ext":"py"},[n("pre",{class:"language-python"},[n("code",null,[e("summary_ids "),n("span",{class:"token operator"},"="),e(" model"),n("span",{class:"token punctuation"},"."),e("generate"),n("span",{class:"token punctuation"},"("),e("inputs"),n("span",{class:"token punctuation"},","),e(" max_length"),n("span",{class:"token operator"},"="),n("span",{class:"token number"},"150"),n("span",{class:"token punctuation"},","),e(" min_length"),n("span",{class:"token operator"},"="),n("span",{class:"token number"},"80"),n("span",{class:"token punctuation"},","),e(" length_penalty"),n("span",{class:"token operator"},"="),n("span",{class:"token number"},"5."),n("span",{class:"token punctuation"},","),e(" num_beams"),n("span",{class:"token operator"},"="),n("span",{class:"token number"},"2"),n("span",{class:"token punctuation"},")"),e(`
`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"})])],-1),ne=n("p",null,"Feel free to replace the values mentioned above with your desired values. Once it’s ready, you can move on to decode the tokenized summary using the tokenizer.decode function:",-1),ee=n("div",{class:"language-python line-numbers-mode","data-ext":"py"},[n("pre",{class:"language-python"},[n("code",null,[e("summary "),n("span",{class:"token operator"},"="),e(" tokenizer"),n("span",{class:"token punctuation"},"."),e("decode"),n("span",{class:"token punctuation"},"("),e("summary_ids"),n("span",{class:"token punctuation"},"["),n("span",{class:"token number"},"0"),n("span",{class:"token punctuation"},"]"),n("span",{class:"token punctuation"},")"),e(`
`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"})])],-1),se=n("p",null,"And there you have it: a text summarizer with Google’s T5. You can replace the texts and values at any time to summarize various arrays of data.",-1),ae=n("p",null,"GPT-3 is a successor to the GPT-2 API and is much more capable and functional. Let’s take a look at how to get it running on Python with an example of downloading PDF research papers.",-1),te=n("p",null,"First, you will need to import all dependencies:",-1),oe=n("div",{class:"language-python line-numbers-mode","data-ext":"py"},[n("pre",{class:"language-python"},[n("code",null,[n("span",{class:"token keyword"},"import"),e(` openai
`),n("span",{class:"token keyword"},"import"),e(` wget
`),n("span",{class:"token keyword"},"import"),e(` pathlib
`),n("span",{class:"token keyword"},"import"),e(` pdfplumber
`),n("span",{class:"token keyword"},"import"),e(" numpy "),n("span",{class:"token keyword"},"as"),e(` np
`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"})])],-1),le=n("p",null,[e("You will then need to install openai to interact with GPT-3, so make sure you have an API key. You can get one here."),n("br"),e(" You will also need wget to download PDFs from the internet. This will further require pdfplumber to convert it back to text. Install all three with pip:")],-1),ie=n("div",{class:"language-bash line-numbers-mode","data-ext":"sh"},[n("pre",{class:"language-bash"},[n("code",null,[e("pip "),n("span",{class:"token function"},"install"),e(` openai
pip `),n("span",{class:"token function"},"install"),e(),n("span",{class:"token function"},"wget"),e(`
pip `),n("span",{class:"token function"},"install"),e(` pdfplumber
`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"})])],-1),re=n("p",null,"To download the PDF and return its local path, enter the following:",-1),ce=n("div",{class:"language-python line-numbers-mode","data-ext":"py"},[n("pre",{class:"language-python"},[n("code",null,[n("span",{class:"token keyword"},"def"),e(),n("span",{class:"token function"},"getPaper"),n("span",{class:"token punctuation"},"("),e("paper_url"),n("span",{class:"token punctuation"},","),e(" filename"),n("span",{class:"token operator"},"="),n("span",{class:"token string"},'"random_paper.pdf"'),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},":"),e(`
    `),n("span",{class:"token triple-quoted-string string"},`"""
    Downloads a paper from the given url and returns
    the local path to that file.
    """`),e(`
    
    downloadedPaper `),n("span",{class:"token operator"},"="),e(" wget"),n("span",{class:"token punctuation"},"."),e("download"),n("span",{class:"token punctuation"},"("),e("paper_url"),n("span",{class:"token punctuation"},","),e(" filename"),n("span",{class:"token punctuation"},")"),e(`
    downloadedPaperFilePath `),n("span",{class:"token operator"},"="),e(" pathlib"),n("span",{class:"token punctuation"},"."),e("Path"),n("span",{class:"token punctuation"},"("),e("downloadedPaper"),n("span",{class:"token punctuation"},")"),e(`
    
    `),n("span",{class:"token keyword"},"return"),e(` downloadedPaperFilePath
`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"})])],-1),pe=n("p",null,"Now, you need to convert the PDF into text so GPT-3 can read it:",-1),ue=n("div",{class:"language-python line-numbers-mode","data-ext":"py"},[n("pre",{class:"language-python"},[n("code",null,[e("paperFilePath "),n("span",{class:"token operator"},"="),e(),n("span",{class:"token string"},'"random_paper.pdf"'),e(`
paperContent `),n("span",{class:"token operator"},"="),e(" pdfplumber"),n("span",{class:"token punctuation"},"."),n("span",{class:"token builtin"},"open"),n("span",{class:"token punctuation"},"("),e("paperFilePath"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},"."),e(`pages

`),n("span",{class:"token keyword"},"def"),e(),n("span",{class:"token function"},"displayPaperContent"),n("span",{class:"token punctuation"},"("),e("paperContent"),n("span",{class:"token punctuation"},","),e(" page_start"),n("span",{class:"token operator"},"="),n("span",{class:"token number"},"0"),n("span",{class:"token punctuation"},","),e(" page_end"),n("span",{class:"token operator"},"="),n("span",{class:"token number"},"5"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},":"),e(`
    `),n("span",{class:"token keyword"},"for"),e(" page "),n("span",{class:"token keyword"},"in"),e(" paperContent"),n("span",{class:"token punctuation"},"["),e("page_start"),n("span",{class:"token punctuation"},":"),e("page_end"),n("span",{class:"token punctuation"},"]"),n("span",{class:"token punctuation"},":"),e(`
        `),n("span",{class:"token keyword"},"print"),n("span",{class:"token punctuation"},"("),e("page"),n("span",{class:"token punctuation"},"."),e("extract_text"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},")"),e(`

displayPaperContent`),n("span",{class:"token punctuation"},"("),e("paperContent"),n("span",{class:"token punctuation"},")"),e(`
`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"})])],-1),de=n("p",null,"Now that you have the text, it’s time to start summarizing it:",-1),me=n("div",{class:"language-python line-numbers-mode","data-ext":"py"},[n("pre",{class:"language-python"},[n("code",null,[n("span",{class:"token keyword"},"def"),e(),n("span",{class:"token function"},"showPaperSummary"),n("span",{class:"token punctuation"},"("),e("paperContent"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},":"),e(`
    tldr_tag `),n("span",{class:"token operator"},"="),e(),n("span",{class:"token string"},'"\\n tl;dr:"'),e(`
    openai`),n("span",{class:"token punctuation"},"."),e("organization "),n("span",{class:"token operator"},"="),e(),n("span",{class:"token string"},"'organization key'"),e(`
    openai`),n("span",{class:"token punctuation"},"."),e("api_key "),n("span",{class:"token operator"},"="),e(),n("span",{class:"token string"},'"your api key"'),e(`
    engine_list `),n("span",{class:"token operator"},"="),e(" openai"),n("span",{class:"token punctuation"},"."),e("Engine"),n("span",{class:"token punctuation"},"."),n("span",{class:"token builtin"},"list"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),e(`
`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"})])],-1),ke=n("p",null,"Here, we are letting the GPT-3 model know that we require a summary. Then, we proceed to set up the environment to use the openai API.",-1),he=n("div",{class:"language-python line-numbers-mode","data-ext":"py"},[n("pre",{class:"language-python"},[n("code",null,[n("span",{class:"token keyword"},"for"),e(" page "),n("span",{class:"token keyword"},"in"),e(" paperContent"),n("span",{class:"token punctuation"},":"),e(`
    text `),n("span",{class:"token operator"},"="),e(" page"),n("span",{class:"token punctuation"},"."),e("extract_text"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),e(),n("span",{class:"token operator"},"+"),e(` tldr_tag
    response `),n("span",{class:"token operator"},"="),e(" openai"),n("span",{class:"token punctuation"},"."),e("Completion"),n("span",{class:"token punctuation"},"."),e("create"),n("span",{class:"token punctuation"},"("),e(`
        engine`),n("span",{class:"token operator"},"="),n("span",{class:"token string"},'"davinci"'),n("span",{class:"token punctuation"},","),e(`
        prompt`),n("span",{class:"token operator"},"="),e("text"),n("span",{class:"token punctuation"},","),e(`
        temperature`),n("span",{class:"token operator"},"="),n("span",{class:"token number"},"0.3"),n("span",{class:"token punctuation"},","),e(` 
        max_tokens`),n("span",{class:"token operator"},"="),n("span",{class:"token number"},"140"),n("span",{class:"token punctuation"},","),e(`
        top_p`),n("span",{class:"token operator"},"="),n("span",{class:"token number"},"1"),n("span",{class:"token punctuation"},","),e(`
        frequency_penalty`),n("span",{class:"token operator"},"="),n("span",{class:"token number"},"0"),n("span",{class:"token punctuation"},","),e(`
        presence_penalty`),n("span",{class:"token operator"},"="),n("span",{class:"token number"},"0"),n("span",{class:"token punctuation"},","),e(`
        stop`),n("span",{class:"token operator"},"="),n("span",{class:"token punctuation"},"["),n("span",{class:"token string"},'"\\n"'),n("span",{class:"token punctuation"},"]"),n("span",{class:"token punctuation"},")"),e(`
    `),n("span",{class:"token keyword"},"print"),n("span",{class:"token punctuation"},"("),e("response"),n("span",{class:"token punctuation"},"["),n("span",{class:"token string"},'"choices"'),n("span",{class:"token punctuation"},"]"),n("span",{class:"token punctuation"},"["),n("span",{class:"token number"},"0"),n("span",{class:"token punctuation"},"]"),n("span",{class:"token punctuation"},"["),n("span",{class:"token string"},'"text"'),n("span",{class:"token punctuation"},"]"),n("span",{class:"token punctuation"},")"),e(`
`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"})])],-1),be=n("p",null,[e("This code extracts the text from each page, feeds the GPT-3 model the max tokens for each page, and prints it to the terminal."),n("br"),e(" Now that everything is set up, we can run the summarizer:")],-1),_e=n("div",{class:"language-python line-numbers-mode","data-ext":"py"},[n("pre",{class:"language-python"},[n("code",null,[e("paperContent "),n("span",{class:"token operator"},"="),e(" pdfplumber"),n("span",{class:"token punctuation"},"."),n("span",{class:"token builtin"},"open"),n("span",{class:"token punctuation"},"("),e("paperFilePath"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},"."),e(`pages
showPaperSummary`),n("span",{class:"token punctuation"},"("),e("paperContent"),n("span",{class:"token punctuation"},")"),e(`
`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"}),n("div",{class:"line-number"})])],-1),ve=n("p",null,[e("Text summarization is very useful for people dealing with large amounts of written data on a daily basis, such as online magazines, research sites, and even for teachers in schools."),n("br"),e(" While there are simple methods of text summarization in Python such as Gensim and Sumy, there are far more powerful but slightly complicated summarizers such as T5 and GPT-3.")],-1),ge=n("p",null,"Which technique to choose really comes down to preference and the use-case for each of these summarizers. But in theory, AI-based summarizers will prove better in the long run as they will constantly learn and provide superior results.",-1),ye={href:"https://www.activestate.com/blog/how-to-do-text-summarization-with-python/",target:"_blank",rel:"noopener noreferrer"},fe=n("div",{class:"hint-container info"},[n("p",{class:"hint-container-title"},"Info"),n("p",null,"Text summarises based on frequency metric")],-1),we={href:"https://www.mygreatlearning.com/blog/text-summarization-in-python/",target:"_blank",rel:"noopener noreferrer"},xe=d(`<div class="hint-container info"><p class="hint-container-title">Not valuable</p><p>Abstractive Summarization</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># Step 1: import the required libraries. </span>
<span class="token keyword">from</span> nltk<span class="token punctuation">.</span>corpus <span class="token keyword">import</span> stopwords
<span class="token keyword">from</span> nltk<span class="token punctuation">.</span>tokenize <span class="token keyword">import</span> word_tokenize<span class="token punctuation">,</span> sent_tokenize

<span class="token comment"># Step 2: Remove the Stop Words and store them in a separate array of words.</span>
<span class="token comment">#Stop Words</span>
<span class="token comment">#  Words such as is, an, a, the, and ‘for‘ do not add value to the meaning of a sentence. For example, let us take a look at the following sentence:</span>
<span class="token comment">#  GreatLearning is one of the most valuable websites for ArtificialIntelligence aspirants.</span>
<span class="token comment">#  After removing the stop words in the above sentence, we can narrow the number of words and preserve the meaning as follows:</span>
<span class="token comment">#  [‘GreatLearning’, ‘one’, ‘useful’, ‘website’, ‘ArtificialIntelligence‘, ‘aspirants’, ‘.’]</span>

<span class="token comment"># Step 3: create a frequency table of the words.</span>
stopwords <span class="token operator">=</span> <span class="token builtin">set</span> <span class="token punctuation">(</span>stopwords<span class="token punctuation">.</span>words<span class="token punctuation">(</span><span class="token string">&quot;english&quot;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
words <span class="token operator">=</span> word_tokenize<span class="token punctuation">(</span>text<span class="token punctuation">)</span>
freqTable <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># Step 4: We will assign a score to each sentence depending on the words it contains and the frequency table.</span>
sentences <span class="token operator">=</span> sent_tokenize<span class="token punctuation">(</span>text<span class="token punctuation">)</span>
sentenceValue <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># Step 5: Assign a score to compare the sentences within the text.</span>
sumValues <span class="token operator">=</span> <span class="token number">0</span>
<span class="token keyword">for</span> sentence <span class="token keyword">in</span> sentenceValue<span class="token punctuation">:</span>
    sumValues <span class="token operator">+=</span> sentenceValue<span class="token punctuation">[</span>sentence<span class="token punctuation">]</span>
average <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>sumValues <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>sentenceValue<span class="token punctuation">)</span><span class="token punctuation">)</span>

</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></div>`,1),ze={href:"https://pypi.org/project/bert-extractive-summarizer/",target:"_blank",rel:"noopener noreferrer"},Te={href:"https://towardsdatascience.com/summarize-a-text-with-python-continued-bbbbb5d37adb",target:"_blank",rel:"noopener noreferrer"},Pe=n("div",{class:"hint-container info"},[n("p",{class:"hint-container-title"},"Info"),n("p",null,[e("Text Summarization Using "),n("span",{style:{color:"orange","font-weight":"bold"}},"NLTK"),e(".")])],-1),Se={href:"https://www.geeksforgeeks.org/python-text-summarizer/",target:"_blank",rel:"noopener noreferrer"},Le=n("div",{class:"hint-container info"},[n("p",{class:"hint-container-title"},"Info"),n("p",null,[e("Text Summarization Using "),n("span",{style:{color:"orange","font-weight":"bold"}},"NLTK"),e(".")])],-1),Ae={href:"https://cloud.google.com/vertex-ai/docs/samples/aiplatform-sdk-summarization#whats-next",target:"_blank",rel:"noopener noreferrer"},Ge=n("div",{class:"hint-container info"},[n("p",{class:"hint-container-title"},"Info"),n("p",null,[e("Summarization using google generative AI. "),n("span",{style:{color:"orange","font-weight":"bold"}},"(Useful but huge project, it will takes lots of time to validate)")])],-1),Ie={href:"https://github.com/zihangdai/xlnet",target:"_blank",rel:"noopener noreferrer"},Ve=n("div",{class:"hint-container info"},[n("p",{class:"hint-container-title"},"Info"),n("p",null,[e("Valuable project using"),n("span",{style:{color:"orange","font-weight":"bold"}},"XLNet"),e(". It will takes lot of time to validate.")])],-1),Ne={href:"https://medium.com/keyreply/xlnet-a-new-pre-training-method-outperforming-bert-on-20-tasks-b34daeee8edb",target:"_blank",rel:"noopener noreferrer"},qe=n("div",{class:"hint-container info"},[n("p",{class:"hint-container-title"},"Info"),n("p",null,[e("Describe another approach "),n("span",{style:{color:"orange","font-weight":"bold"}},"(XLNet)"),e(" versus to BERT.")])],-1),Fe={href:"https://jalammar.github.io/illustrated-gpt2/",target:"_blank",rel:"noopener noreferrer"},Re=n("div",{class:"hint-container info"},[n("p",{class:"hint-container-title"},"Info"),n("p",null,"Principle introduction")],-1),Ce={href:"https://docs.kanaries.net/articles/google-bard-api",target:"_blank",rel:"noopener noreferrer"};function We(Be,Ee){const a=i("ExternalLinkIcon"),r=i("Tabs");return p(),u("div",null,[n("ol",null,[n("li",null,[n("a",k,[e("Bert Test on Colab "),h,s(a)])]),n("li",null,[n("a",b,[e("OpenAI test on Colab "),_,s(a)])]),n("li",null,[n("a",v,[e("PaLM Test on Colab"),s(a)])])]),g,y,n("table",null,[f,n("tbody",null,[n("tr",null,[w,n("td",null,[n("a",x,[e("Github"),s(a)])]),z]),n("tr",null,[T,n("td",null,[n("a",P,[e("Github"),s(a)])]),S]),n("tr",null,[L,n("td",null,[n("a",A,[e("Github of Bert"),s(a)]),e(),n("a",G,[e("Bert-Summarizer"),s(a)])]),I]),n("tr",null,[V,n("td",null,[n("a",N,[e("Github of KeyBert"),s(a)])]),q]),n("tr",null,[F,n("td",null,[n("a",R,[e("Github Site"),s(a)]),e(),n("a",C,[e("Another Example"),s(a)])]),W]),B,n("tr",null,[E,n("td",null,[n("a",O,[e("Github Site"),s(a)])]),M]),n("tr",null,[D,n("td",null,[n("a",K,[e("Github Site"),s(a)])]),j]),n("tr",null,[H,n("td",null,[n("a",Y,[e("Github Site"),s(a)])]),X]),n("tr",null,[U,n("td",null,[n("a",Z,[e("Github Site"),s(a)])]),Q]),n("tr",null,[J,n("td",null,[n("a",$,[e("Github Site"),s(a)])]),nn]),n("tr",null,[en,n("td",null,[n("a",sn,[e("Github Sample"),s(a)]),n("a",an,[e("openai documentation"),s(a)])]),tn]),n("tr",null,[on,n("td",null,[n("a",ln,[e("AWS bedrock"),s(a)])]),rn]),n("tr",null,[cn,n("td",null,[n("a",pn,[e("PaLM API: Text Quickstart with Python"),s(a)])]),un])])]),dn,mn,n("ol",null,[n("li",null,[n("p",null,[n("a",kn,[e("Text Summarization using BERT, GPT2, XLNet"),s(a)])]),n("div",hn,[bn,_n,n("p",null,[e("Other options are given: "),n("span",vn,[n("a",gn,[e("GPT2"),s(a)]),e(", "),n("a",yn,[e("XLNet"),s(a)])])])])]),n("li",null,[n("p",null,[n("a",fn,[e("BERT Extractive Summarizer vs Word2Vec Extractive Summarizer: Which one is better and faster?"),s(a)])]),n("div",wn,[xn,zn,n("p",null,[e("Other options are given: "),n("span",Tn,[n("a",Pn,[e("TextRank"),s(a)]),e(", "),n("a",Sn,[e("TF-IDF"),s(a)]),e(", and "),n("a",Ln,[e("Word2Vec"),s(a)])])])])]),n("li",null,[n("p",null,[n("a",An,[e("Extractive Summarization with BERT Extractive Summarizer"),s(a)])]),Gn]),n("li",null,[n("p",null,[n("a",In,[e("5 Powerful Text Summarization Techniques in Python"),s(a)])]),s(r,{id:"237",data:[{id:"Gensim"},{id:"Sumy"},{id:"NLTK"},{id:"T5"},{id:"GPT-3"}]},{title0:t(({value:o,isActive:l})=>[e("Gensim")]),title1:t(({value:o,isActive:l})=>[e("Sumy")]),title2:t(({value:o,isActive:l})=>[e("NLTK")]),title3:t(({value:o,isActive:l})=>[e("T5")]),title4:t(({value:o,isActive:l})=>[e("GPT-3")]),tab0:t(({value:o,isActive:l})=>[Vn]),tab1:t(({value:o,isActive:l})=>[Nn,qn,Fn,Rn,Cn,Wn,Bn,En]),tab2:t(({value:o,isActive:l})=>[On,Mn]),tab3:t(({value:o,isActive:l})=>[Dn,Kn,jn,Hn,Yn,Xn,Un,Zn,Qn,Jn,$n,ne,ee,se]),tab4:t(({value:o,isActive:l})=>[ae,te,oe,le,ie,re,ce,pe,ue,de,me,ke,he,be,_e,ve,ge]),_:1})]),n("li",null,[n("p",null,[n("a",ye,[e("How to do text summarization with deep learning and Python"),s(a)])]),fe]),n("li",null,[n("p",null,[n("a",we,[e("Text Summarization in Python"),s(a)])]),xe]),n("li",null,[n("p",null,[n("a",ze,[e("Bert Offical Website"),s(a)])])]),n("li",null,[n("p",null,[n("a",Te,[e("Summarize a Text with Python — Continued"),s(a)])]),Pe]),n("li",null,[n("p",null,[n("a",Se,[e("Python | Text Summarizer"),s(a)])]),Le]),n("li",null,[n("p",null,[n("a",Ae,[e("Summarize text content using Generative AI (Generative AI)"),s(a)])]),Ge]),n("li",null,[n("p",null,[n("a",Ie,[e("Github XLNet(zihangdai)"),s(a)])]),Ve]),n("li",null,[n("p",null,[n("a",Ne,[e("XLNet — A new pre-training method outperforming BERT on 20 tasks"),s(a)])]),qe]),n("li",null,[n("p",null,[n("a",Fe,[e("The Illustrated GPT-2 (Visualizing Transformer Language Models)"),s(a)])]),Re]),n("li",null,[n("p",null,[n("a",Ce,[e("Harnessing the Power of Google Bard with Python: A Comprehensive Guide"),s(a)])])])])])}const De=c(m,[["render",We],["__file","Summarization.html.vue"]]);export{De as default};
