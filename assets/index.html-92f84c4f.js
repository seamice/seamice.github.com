const e=JSON.parse(`{"key":"v-3bca2338","path":"/techniques/AI/peft/","title":"PEFT","lang":"en-US","frontmatter":{"title":"PEFT","index":false,"icon":"list-check","author":"Haiyue","category":["readme"],"description":"PEFT (Parameter-Efficient Fine-Tuning) is a library for efficiently adapting large pretrained models to various downstream applications without fine-tuning all of a model’s parameters because it is prohibitively costly. PEFT methods only fine-tune a small number of (extra) model parameters - significantly decreasing computational and storage costs - while yielding performance comparable to a fully fine-tuned model. This makes it more accessible to train and store large language models (LLMs) on consumer hardware.","head":[["meta",{"property":"og:url","content":"https://seamice.github.io/blog/techniques/AI/peft/"}],["meta",{"property":"og:site_name","content":"Haiyue's Blog"}],["meta",{"property":"og:title","content":"PEFT"}],["meta",{"property":"og:description","content":"PEFT (Parameter-Efficient Fine-Tuning) is a library for efficiently adapting large pretrained models to various downstream applications without fine-tuning all of a model’s parameters because it is prohibitively costly. PEFT methods only fine-tune a small number of (extra) model parameters - significantly decreasing computational and storage costs - while yielding performance comparable to a fully fine-tuned model. This makes it more accessible to train and store large language models (LLMs) on consumer hardware."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2024-01-05T12:58:37.000Z"}],["meta",{"property":"article:author","content":"Haiyue"}],["meta",{"property":"article:modified_time","content":"2024-01-05T12:58:37.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"PEFT\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2024-01-05T12:58:37.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Haiyue\\"}]}"]]},"headers":[],"git":{"createdTime":1704459517000,"updatedTime":1704459517000,"contributors":[{"name":"Haiyue","email":"nutterair1989@gmail.com","commits":1}]},"readingTime":{"minutes":0.43,"words":128},"filePathRelative":"techniques/AI/peft/README.md","localizedDate":"January 5, 2024","excerpt":"<p>PEFT (Parameter-Efficient Fine-Tuning) is a library for efficiently <span style=\\"color:orange\\">adapting large pretrained models</span> to various downstream applications <span style=\\"color:orange\\">without fine-tuning all of a model’s parameters</span> because it is prohibitively costly. PEFT methods only <span style=\\"color:orange\\">fine-tune a small number of (extra) model parameters</span> - significantly decreasing computational and storage costs - while yielding performance comparable to a fully fine-tuned model. This makes it more accessible to train and store large language models (LLMs) on consumer hardware.</p>","autoDesc":true}`);export{e as data};
