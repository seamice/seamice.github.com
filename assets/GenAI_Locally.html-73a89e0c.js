import{_ as n}from"./plugin-vue_export-helper-c27b6911.js";import{r as t,o as s,c as l,b as e,d as o,e as a}from"./app-dbe05420.js";const c={},i=e("h2",{id:"previous-work",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#previous-work","aria-hidden":"true"},"#"),o(" Previous work")],-1),_={href:"https://colab.research.google.com/drive/11vliXWcRSSwZe027Z5wMn7ne7TH3H64j?usp=sharing",target:"_blank",rel:"noopener noreferrer"},h=e("h2",{id:"resources",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#resources","aria-hidden":"true"},"#"),o(" Resources")],-1),d={href:"https://lachieslifestyle.com/2023/07/29/how-to-install-llama-2/",target:"_blank",rel:"noopener noreferrer"},p=e("br",null,null,-1),m=e("a",{href:""},"lmstudio.ai",-1),u=e("br",null,null,-1),f={href:"https://aws.amazon.com/blogs/aws/amazon-bedrock-now-provides-access-to-llama-2-chat-13b-model/",target:"_blank",rel:"noopener noreferrer"},k={href:"https://pypi.org/project/llama-server/",target:"_blank",rel:"noopener noreferrer"};function b(v,w){const r=t("ExternalLinkIcon");return s(),l("div",null,[i,e("p",null,[e("a",_,[o("Event Ranking"),a(r)])]),h,e("p",null,[e("a",d,[o("How to use LLama Model locally"),a(r)]),p,m,u,e("a",f,[o("Llama 2 Chat 13B now supported in Amazon Bedrock"),a(r)])]),e("p",null,[e("a",k,[o("llama-server"),a(r)])])])}const L=n(c,[["render",b],["__file","GenAI_Locally.html.vue"]]);export{L as default};
